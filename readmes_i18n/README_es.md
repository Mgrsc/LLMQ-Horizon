<div align="center">

<img src="static/LLMQ.webp" width="400" style="margin-bottom: 10px;">

# ü§ñ LLMQ-Horizon Bot de Chat QQ

**Un bot inteligente de QQ basado en NoneBot2 y LangGraph, que admite conversaciones con m√∫ltiples modelos, invocaci√≥n de herramientas y gesti√≥n de sesiones**

<br>

**Las herramientas est√°n escritas utilizando Function-calling, sin usar complementos, consulta [OpenAI Function Calling](https://platform.openai.com/docs/guides/function-calling) y [LangChain Tools](https://python.langchain.com/docs/how_to/#tools)**

<br>

[![FOSSA Status](https://app.fossa.com/api/projects/git%2Bgithub.com%2FMgrsc%2FLLMQ-Horizon.svg?type=small)](https://app.fossa.com/projects/git%2Bgithub.com%2FMgrsc%2FLLMQ-Horizon?ref=badge_small)
[![Docker Release](https://img.shields.io/docker/pulls/bitfennec/llmq-horizon?color=%230077c8&label=Docker%20Pulls&logo=docker&logoColor=white&style=flat)](https://hub.docker.com/r/bitfennec/llmq-horizon)
[![License](https://img.shields.io/github/license/Mgrsc/LLMQ-Horizon?color=%2300c853&label=MIT%20License&style=flat)](https://github.com/Mgrsc/LLMQ-Horizon/blob/main/LICENSE)

</div>

---

## ‚ú® Caracter√≠sticas Principales

-   **üîå Integraci√≥n de Herramientas Ricas:** Ejecuci√≥n de c√≥digo, consulta del clima, adivinaci√≥n, pintura, etc.
-   **ü§ñ Soporte para M√∫ltiples Modelos Grandes:** OpenAI, Google Gemini, Groq, etc.
-   **üí¨ Gesti√≥n Completa de Conversaciones:** Chat grupal/privado, conversaciones de m√∫ltiples turnos, aislamiento de sesiones.
-   **üéØ Formas Flexibles de Activaci√≥n:** @, palabras clave, prefijos de comandos.
-   **üé® Capacidades Multimedia:** An√°lisis de im√°genes, procesamiento de audio y video.
-   **‚ö° Gesti√≥n Autom√°tica de Sesiones:** Limpieza por tiempo de espera, control de concurrencia.
-   **ü¶ñ Potente Capacidad de Extensi√≥n:** Posibilidad de escribir herramientas propias, posibilidad de usar herramientas para controlar Nonebot.

---

## üöÄ Inicio R√°pido

### 1. Preparaci√≥n del Entorno de Implementaci√≥n

-   Docker y Docker Compose
-   Entorno de red estable
-   Sistemas recomendados: Ubuntu 22.04 y superior, Debian 11 y superior

### 2. Pasos de Instalaci√≥n

```bash
# 1. Clonar el proyecto
git clone https://github.com/Mgrsc/LLMQ-Horizon.git
cd LLMQ-Horizon

# 2. Preparar los archivos de configuraci√≥n
cp config-tools.toml.example config-tools.toml
cp config.toml.example config.toml
cd napcat/config/
mv onebot11_qq.json onebot11_<tu_QQ>.json  # Reemplaza con tu n√∫mero de QQ real

# 3. Modificar la configuraci√≥n (consulta los comentarios en los archivos de configuraci√≥n para realizar las modificaciones)
vim config.toml
vim config-tools.toml

# 4. Iniciar el servicio
docker compose up -d

# 5. Escanear el c√≥digo para iniciar sesi√≥n
docker compose logs -f

# Reiniciar el servicio LLMQ
docker compose restart llmq

# Detener todos los servicios
docker compose down
```

## üõ†Ô∏è Configuraci√≥n de Herramientas

<details>
<summary>üíª Ejecuci√≥n de C√≥digo (Code Runner - Judge0)</summary>

[Tutorial oficial de implementaci√≥n de Judge0](https://github.com/judge0/judge0/blob/master/CHANGELOG.md)

1. **Prepara un entorno Ubuntu 22.04 o superior y Docker, configura cgroup v1:**

    ```bash
    sudo sed -i 's/GRUB_CMDLINE_LINUX=""/GRUB_CMDLINE_LINUX="systemd.unified_cgroup_hierarchy=0"/' /etc/default/grub
    sudo update-grub
    sudo reboot
    ```

2. **Implementa Judge0:**

    ```bash
    wget https://github.com/judge0/judge0/releases/download/v1.13.1/judge0-v1.13.1.zip
    unzip judge0-v1.13.1.zip
    cd judge0-v1.13.1

    # Genera dos contrase√±as y config√∫ralas
    openssl rand -hex 32

    # Usa las contrase√±as generadas para actualizar las variables REDIS_PASSWORD y POSTGRES_PASSWORD en el archivo judge0.conf.

    # Inicia el servicio
    docker-compose up -d db redis
    sleep 10s
    docker-compose up -d
    sleep 5s
    ```

    Tu instancia de Judge0 CE v1.13.1 ahora est√° activa y en funcionamiento; accede a http://<tu_direcci√≥n_IP_del_servidor>:2358/docs para obtener la documentaci√≥n.

3. **Configura config-tools.toml:**

    ```toml
    [code_runner]
    judge0_url = "http://tu-servidor:2358"
    judge0_api_key = "tu-api-key"
    ```

</details>

<details>
<summary>üòé Notas (memos_manage - Memos)</summary>

[Tutorial oficial de implementaci√≥n de Memos](https://www.usememos.com/docs/install/container-install)

1. **Prepara un entorno Ubuntu 22.04 o superior y Docker:**

2. **Escribe el archivo docker-compose.yaml**

    ```yaml
    services:
      memos:
        image: neosmemo/memos:stable
        container_name: memos
        ports:
          - 5230:5230
        volumes:
          - ./memos:/var/opt/memos
        restart: always
    ```

3. **Inicia Memos**

    ```shell
    docker compose up -d
    ```

    Ahora puedes acceder a Memos en http://<tu_direcci√≥n_IP_del_servidor>:5230, y obtener los Tokens en la configuraci√≥n de Memos.

4. **Rellena el archivo de configuraci√≥n**

    ```toml
    [memos]
    url = "http://tu-servidor:xxx"
    memos_token = "<introduce_los_tokens_obtenidos>"
    default_visibility = "PRIVATE"
    page_size = 10
    user_id = 6
    ```

</details>

## üìù Descripci√≥n de Comandos

| Comando                      | Descripci√≥n                             |
| :------------------------ | :-------------------------------------- |
| `/chat model <nombre_modelo>`   | Cambia el modelo de conversaci√≥n        |
| `/chat clear`             | Limpia todas las conversaciones          |
| `/chat group <true/false>` | Activa/desactiva el aislamiento de grupos |
| `/chat down`              | Desactiva la funci√≥n de conversaci√≥n     |
| `/chat up`                | Activa la funci√≥n de conversaci√≥n       |
| `/chat chunk <true/false>` | Activa/desactiva el env√≠o en fragmentos  |

## ‚ùó Preguntas Frecuentes

<details>
<summary>1. Fallo al iniciar sesi√≥n</summary>

-   Verifica que la configuraci√≥n del n√∫mero de QQ sea correcta.
-   Confirma el formato del archivo de configuraci√≥n de napcat.
-   Consulta los registros del contenedor napcat para solucionar el problema.

</details>

<details>
<summary>2. Fallo al invocar herramientas</summary>

-   Confirma que el modelo admita la capacidad de invocaci√≥n de funciones.
-   Verifica la configuraci√≥n de las claves API relacionadas.
-   Consulta los registros del contenedor LLMQ para localizar el error.
-   En el contenedor docker, a√±ade [LangSmith](https://smith.langchain.com/) para depurar.

    ```yaml
    environment:
      - LANGCHAIN_TRACING_V2=true
      - LANGCHAIN_ENDPOINT="https://api.smith.langchain.com"
      - LANGCHAIN_API_KEY="<tu_clave_api>"
      - LANGCHAIN_PROJECT="<tu_nombre_de_proyecto>"
    ```

</details>

<details>
<summary>3. Otros problemas</summary>

-   Para otros problemas, por favor √∫nete al grupo de QQ para discutir
    ![qrcode](static/qrcode.jpg)

</details>

## üîó Proyectos Relacionados

-   [NoneBot2](https://github.com/nonebot/nonebot2)
-   [LangGraph](https://github.com/langchain-ai/langgraph)
-   [LangChain](https://github.com/langchain-ai/langchain)
-   [Judge0](https://github.com/judge0/judge0)
-   [Memos](https://github.com/usememos/memos)
-   [NapCat](https://github.com/NapNeko/NapCatQQ)

## üìÑ Licencia

[![FOSSA Status](https://app.fossa.com/api/projects/git%2Bgithub.com%2FMgrsc%2FLLMQ-Horizon.svg?type=large&issueType=license)](https://app.fossa.com/projects/git%2Bgithub.com%2FMgrsc%2FLLMQ-Horizon?ref=badge_large&issueType=license)

Este proyecto se distribuye bajo la [Licencia MIT](https://github.com/Mgrsc/LLMQ-Horizon/blob/main/LICENSE).

Copyright ¬© 2024 Bitfennec.

---