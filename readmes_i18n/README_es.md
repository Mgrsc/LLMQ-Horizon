<div align="center">

<img src="static/LLMQ.webp" width="400" style="margin-bottom: 10px;">

# ü§ñ LLMQ-Horizon Bot de chat QQ

**Bot inteligente de QQ basado en NoneBot2 y LangGraph, que admite conversaciones con m√∫ltiples modelos, llamadas a herramientas y gesti√≥n de sesiones**

<br>

**Las herramientas est√°n escritas utilizando Function-calling, sin usar plugins, siguiendo [OpenAI Function Calling](https://platform.openai.com/docs/guides/function-calling) , [LangChain Tools](https://python.langchain.com/docs/how_to/#tools)**

<br>

[![FOSSA Status](https://app.fossa.com/api/projects/git%2Bgithub.com%2FMgrsc%2FLLMQ-Horizon.svg?type=small)](https://app.fossa.com/projects/git%2Bgithub.com%2FMgrsc%2FLLMQ-Horizon?ref=badge_small)
[![Docker Release](https://img.shields.io/docker/pulls/bitfennec/llmq-horizon?color=%230077c8&label=Docker%20Pulls&logo=docker&logoColor=white&style=flat)](https://hub.docker.com/r/bitfennec/llmq-horizon)
[![License](https://img.shields.io/github/license/Mgrsc/LLMQ-Horizon?color=%2300c853&label=MIT%20License&style=flat)](https://github.com/Mgrsc/LLMQ-Horizon/blob/main/LICENSE)

<br>

[English](https://github.com/Mgrsc/LLMQ-Horizon/blob/main/readmes_i18n/README_en.md) | [Deutsch](https://github.com/Mgrsc/LLMQ-Horizon/blob/main/readmes_i18n/README_de.md) | [Espa√±ol](https://github.com/Mgrsc/LLMQ-Horizon/blob/main/readmes_i18n/README_es.md) | [Fran√ßais](https://github.com/Mgrsc/LLMQ-Horizon/blob/main/readmes_i18n/README_fr.md) | [Êó•Êú¨Ë™û](https://github.com/Mgrsc/LLMQ-Horizon/blob/main/readmes_i18n/README_ja.md)

</div>

---

## ‚ú® Caracter√≠sticas Principales

-   **üîå Rica Integraci√≥n de Herramientas:** Ejecuci√≥n de c√≥digo, consulta del clima, adivinaci√≥n, dibujo, etc.
-   **ü§ñ Soporte para M√∫ltiples Modelos Grandes:** OpenAI, Google Gemini, Groq, etc.
-   **üí¨ Gesti√≥n Completa de Conversaciones:** Chats grupales/privados, conversaciones de m√∫ltiples turnos, aislamiento de sesiones.
-   **üéØ M√©todos de Activaci√≥n Flexibles:** @, palabras clave, prefijos de comandos.
-   **üé® Capacidades Multimedia:** An√°lisis de im√°genes, procesamiento de audio y video.
-   **‚ö° Gesti√≥n Autom√°tica de Sesiones:** Limpieza por tiempo de espera, control de concurrencia.
-   **ü¶ñ Potente Capacidad de Expansi√≥n:** Posibilidad de escribir herramientas propias, posibilidad de usar herramientas para controlar nonebot.

---

## üöÄ Inicio R√°pido

### 1. Preparaci√≥n del Entorno de Implementaci√≥n

-   Docker y Docker Compose
-   Entorno de red estable
-   Sistemas recomendados: Ubuntu 22.04 y superior, Debian 11 y superior

### 2. Pasos de Instalaci√≥n

```bash
# 1. Clonar el proyecto
git clone https://github.com/Mgrsc/LLMQ-Horizon.git
cd LLMQ-Horizon

# 2. Preparar el archivo de configuraci√≥n
cp config-tools.toml.example config-tools.toml
cp config.toml.example config.toml
cd napcat/config/
mv onebot11_qq.json onebot11_<tu_QQ>.json  # Reemplazar con el n√∫mero de QQ real

# 3. Modificar la configuraci√≥n (consultar los comentarios en los archivos de configuraci√≥n)
vim config.toml
vim config-tools.toml

# 4. Iniciar el servicio
docker compose up -d

# 5. Iniciar sesi√≥n escaneando el c√≥digo QR
docker compose logs -f

# Reiniciar el servicio LLMQ
docker compose restart llmq

# Detener todos los servicios
docker compose down
```

## üõ†Ô∏è Configuraci√≥n de Herramientas

<details>
<summary>üíª Ejecuci√≥n de C√≥digo (Code Runner - Judge0)</summary>

[Tutorial de implementaci√≥n oficial de Judge0](https://github.com/judge0/judge0/blob/master/CHANGELOG.md)

1. **Preparar un entorno Ubuntu 22.04 o superior y Docker, configurar cgroup v1:**

    ```bash
    sudo sed -i 's/GRUB_CMDLINE_LINUX=""/GRUB_CMDLINE_LINUX="systemd.unified_cgroup_hierarchy=0"/' /etc/default/grub
    sudo update-grub
    sudo reboot
    ```

2. **Implementar Judge0:**

    ```bash
    wget https://github.com/judge0/judge0/releases/download/v1.13.1/judge0-v1.13.1.zip
    unzip judge0-v1.13.1.zip
    cd judge0-v1.13.1

    # Generar dos contrase√±as y establecer contrase√±as
    openssl rand -hex 32

    # Usar las contrase√±as generadas para actualizar las variables REDIS_PASSWORD y POSTGRES_PASSWORD en el archivo judge0.conf.

    # Iniciar el servicio
    docker-compose up -d db redis
    sleep 10s
    docker-compose up -d
    sleep 5s
    ```

    Su instancia Judge0 CE v1.13.1 ahora est√° iniciada y en funcionamiento; visite http://<su_direcci√≥n_IP_del_servidor>:2358/docs para obtener la documentaci√≥n.

3. **Configurar config-tools.toml:**

    ```toml
    [code_generation_running]
    judge0_url = "http://tu-servidor:2358"
    judge0_api_key = "tu-api-key"
    ```

</details>

<details>
<summary>üòé Notas (memos_manage - Memos)</summary>

[Tutorial de implementaci√≥n oficial de Memos](https://www.usememos.com/docs/install/container-install)

1. **Preparar un entorno Ubuntu 22.04 o superior y Docker:**

2. **Escribir el archivo docker-compose.yaml**

    ```yaml
    services:
      memos:
        image: neosmemo/memos:stable
        container_name: memos
        ports:
          - 5230:5230
        volumes:
          - ./memos:/var/opt/memos
        restart: always
    ```

3. **Iniciar memos**

    ```shell
    docker compose up -d
    ```

    Ahora puede acceder a memos en http://<su_direcci√≥n_IP_del_servidor>:5230, y obtener Tokens en Settings de memos.

4. **Completar el archivo de configuraci√≥n**

    ```toml
    [memos]
    url = "http://tu-servidor:xxx"
    memos_token = "<ingresar_los_tokens_obtenidos>"
    default_visibility = "PRIVATE"
    page_size = 10
    user_id = 6
    ```

</details>

## üìù Descripci√≥n de Comandos

| Comando                      | Descripci√≥n                                  |
| :--------------------------- | :------------------------------------------- |
| `/chat model <nombre_modelo>` | Cambiar el modelo de conversaci√≥n             |
| `/chat clear`               | Limpiar todas las conversaciones              |
| `/chat group <true/false>`   | Activar/desactivar el aislamiento de chats grupales |
| `/chat down`                | Desactivar la funci√≥n de conversaci√≥n        |
| `/chat up`                  | Activar la funci√≥n de conversaci√≥n          |
| `/chat chunk <true/false>`  | Activar/desactivar el env√≠o segmentado         |

## ü¶ä T√©cnicas para la Redacci√≥n de Prompts

<details>
<summary>1. Principios B√°sicos</summary>

- Instrucciones Claras: Utilizar lenguaje imperativo para indicar claramente las necesidades del usuario, asegurando que el LLM comprenda con precisi√≥n.
- Proporcionar Ejemplos/Textos de Referencia: Proporcionar ejemplos e informaci√≥n detallada, constituyendo un Prompt Few-shot, que ayude al LLM a fortalecer la comprensi√≥n de la intenci√≥n.
- Expresi√≥n Estructurada: Utilizar s√≠mbolos de marcado (como etiquetas XML, triple comillas, Markdown) para mejorar la legibilidad y hacer que la expresi√≥n del prompt sea clara.
- Control de Salida: Especificar los requisitos de formato de salida, estilo de lenguaje, etc., para asegurar que el LLM genere una salida que cumpla con las expectativas del usuario.
- Optimizaci√≥n del Dise√±o: Organizar cuidadosamente el dise√±o de la disposici√≥n del Prompt, para que el LLM lo comprenda f√°cilmente.
</details>
<details>
<summary>2. Otras T√©cnicas</summary>

- Listar las herramientas disponibles, con explicaciones y requisitos para las herramientas complejas
  ```
  create_speech generar voz
    - M√°ximo 40 palabras, no se pueden a√±adir emojis
    - Idiomas soportados: chino, ingl√©s, japon√©s, alem√°n, franc√©s, espa√±ol, coreano, √°rabe, ruso, holand√©s, italiano, polaco, portugu√©s
    - Mapeo de voces disponibles:
        Keli = keli
        Sigewen = xigewen
        Shenzi = shenzi
        Dingzhen = dingzhen
        Leijun = leijun
        Lanyangyang = lanyangyang
  ```
- Requerir el env√≠o de la direcci√≥n file:// retornada por la herramienta
  ```
    El dibujo, la obtenci√≥n de m√∫sica y el tts deben enviar al usuario el enlace o la direcci√≥n de la ruta del archivo retornada
  ```
- Ejemplos de maquetaci√≥n del contenido devuelto por la herramienta
  ```
    # Ejemplo de optimizaci√≥n de la maquetaci√≥n del contenido devuelto por la herramienta
    Ejemplo de formato de datos devueltos por get_weather_data:
    * A: Dime el clima de Changsha hoy
        T: Llama a la herramienta `get_weather_data` para obtener el clima
        Q:
        üå§Ô∏è Clima de {ubicaci√≥n}
        üåÖ Amanecer y atardecer: {xx:xx}-{xx:xx sin a√±o}
        ‚è±Ô∏è Hora: {hora}
        üå°Ô∏è Temperatura: {temperatura}‚ÑÉ
        üíß Humedad: {humedad}%
        üß£ Sensaci√≥n t√©rmica: {sensaci√≥n t√©rmica}‚ÑÉ
        üçÉ Direcci√≥n y velocidad del viento: {direcci√≥n del viento}-{velocidad del viento}
        üìã Condici√≥n general: {an√°lisis integral}
        Cari√±o, deber√≠as ponerte m√°s ropa cuando salgas~ Ten cuidado de no resfriarte
  ```
</details>

## ‚ùó Preguntas Frecuentes

Todas las herramientas se han probado. Si hay alg√∫n problema, consulte las siguientes comprobaciones.

<details>
<summary>1. Error al iniciar sesi√≥n</summary>

-   Comprobar si la configuraci√≥n del n√∫mero de QQ es correcta
-   Confirmar el formato del archivo de configuraci√≥n de napcat
-   Ver los logs del contenedor de napcat para solucionar problemas

</details>

<details>
<summary>2. Error al llamar a las herramientas</summary>

-   Confirmar que el modelo admite la funci√≥n de llamada a funciones
-   Comprobar la configuraci√≥n de las claves API relacionadas
-   Ver los logs del contenedor LLMQ para ubicar los errores
-   A√±adir [LangSmith](https://smith.langchain.com/) en el contenedor docker para depurar

    ```yaml
    environment:
      - LANGCHAIN_TRACING_V2=true
      - LANGCHAIN_ENDPOINT="https://api.smith.langchain.com"
      - LANGCHAIN_API_KEY="<tu_api_key>"
      - LANGCHAIN_PROJECT="<nombre_de_tu_proyecto>"
    ```

</details>

<details>
<summary>3. Otros problemas</summary>

-   Para otros problemas, √∫nete al grupo QQ para discutir
    ![qrcode](static/qrcode.jpg)

</details>

## üîó Proyectos Relacionados

-   [NoneBot2](https://github.com/nonebot/nonebot2)
-   [LangGraph](https://github.com/langchain-ai/langgraph)
-   [LangChain](https://github.com/langchain-ai/langchain)
-   [Judge0](https://github.com/judge0/judge0)
-   [Memos](https://github.com/usememos/memos)
-   [NapCat](https://github.com/NapNeko/NapCatQQ)

## üìÑ Licencia

[![FOSSA Status](https://app.fossa.com/api/projects/git%2Bgithub.com%2FMgrsc%2FLLMQ-Horizon.svg?type=large&issueType=license)](https://app.fossa.com/projects/git%2Bgithub.com%2FMgrsc%2FLLMQ-Horizon?ref=badge_large&issueType=license)

Este proyecto est√° licenciado bajo la [Licencia MIT](https://github.com/Mgrsc/LLMQ-Horizon/blob/main/LICENSE).

Copyright ¬© 2024 Bitfennec.

---